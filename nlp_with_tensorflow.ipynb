{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinuka-kasun-medis/mycolab/blob/main/nlp_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MFt16hGWCntt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "nltk.download('words')\n",
        "english_words = words.words()\n",
        "\n",
        "def generate_misspelled_words(words_list, num_misspelled=1):\n",
        "    misspelled_words = []\n",
        "    for word in words_list:\n",
        "        for _ in range(num_misspelled):\n",
        "            random_index = np.random.randint(len(word))\n",
        "            misspelled_word = word[:random_index] + word[random_index + 1:]\n",
        "            misspelled_words.append((misspelled_word, word))\n",
        "    return misspelled_words\n",
        "\n",
        "misspelled_data = generate_misspelled_words(english_words, num_misspelled=1)\n",
        "misspelled_words, correct_words = zip(*misspelled_data)\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(correct_words)\n",
        "\n",
        "misspelled_sequences = tokenizer.texts_to_sequences(misspelled_words)\n",
        "correct_sequences = tokenizer.texts_to_sequences(correct_words)\n",
        "\n",
        "max_seq_length = max(max(len(seq) for seq in misspelled_sequences),\n",
        "                    max(len(seq) for seq in correct_sequences))\n",
        "\n",
        "misspelled_sequences = pad_sequences(misspelled_sequences, maxlen=max_seq_length, padding='post')\n",
        "correct_sequences = pad_sequences(correct_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6E2uzkRXO2t",
        "outputId": "49856fa4-d2a6-4627-8cf8-5f4976cb658d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify data shapes and types\n",
        "print(\"Shape of misspelled_sequences:\", misspelled_sequences.shape)\n",
        "print(\"Shape of correct_sequences:\", correct_sequences.shape)\n",
        "print(\"Data type of correct_sequences:\", correct_sequences.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJsJWFonXU3F",
        "outputId": "52821c79-f0ea-45ec-fb57-2ebd495fbe1f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of misspelled_sequences: (236736, 24)\n",
            "Shape of correct_sequences: (236736, 24)\n",
            "Data type of correct_sequences: int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Embedding(vocab_size, 16, input_length=max_seq_length),\n",
        "#     tf.keras.layers.LSTM(64),\n",
        "#     tf.keras.layers.Dense(vocab_size, activation='softmax')  # Output units equal to vocab_size\n",
        "# ])\n",
        "\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 16, input_length=max_seq_length),\n",
        "    tf.keras.layers.LSTM(64, return_sequences=True),  # Set return_sequences=True for TimeDistributed\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uwEfAKNeXfZH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the target labels to integer type (required for sparse_categorical_crossentropy)\n",
        "correct_sequences = correct_sequences.astype(np.int32)\n"
      ],
      "metadata": {
        "id": "Vest1HLYXil1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "model.fit(misspelled_sequences, correct_sequences, epochs=1, batch_size=32, validation_split=0.2) #me line eka."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1EQfcW8YL8t",
        "outputId": "f884b0f7-ed70-4cac-c30e-7584874fcd46"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5919/5919 [==============================] - 84s 14ms/step - loss: 0.4842 - accuracy: 0.8496 - val_loss: 0.4772 - val_accuracy: 0.8379\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e6d0c139ab0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_word(word, tokenizer, max_seq_length):\n",
        "    # Tokenize the input word into character-level sequence\n",
        "    word_sequence = tokenizer.texts_to_sequences([word])\n",
        "    # Pad the sequence to match the model's input length\n",
        "    padded_sequence = pad_sequences(word_sequence, maxlen=max_seq_length, padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "def sequences_to_text(sequence, tokenizer):\n",
        "    # Convert the sequence back to the word\n",
        "    word = tokenizer.sequences_to_texts([sequence])[0]\n",
        "    return word\n",
        "\n",
        "def spell_check(word, tokenizer, max_seq_length):\n",
        "    # Preprocess the input word\n",
        "    input_sequence = preprocess_word(word, tokenizer, max_seq_length)\n",
        "    # Predict the corrected word using the model\n",
        "    predicted_sequence = model.predict(input_sequence)\n",
        "    # Convert the sequence back to the word\n",
        "    corrected_word = sequences_to_text(predicted_sequence[0], tokenizer)\n",
        "    return corrected_word\n",
        "\n",
        "# Example usage\n",
        "input_word = \"bannna\"\n",
        "corrected_word = spell_check(input_word, tokenizer, max_seq_length)\n",
        "print(f\"Original word: {input_word}\")\n",
        "print(f\"Corrected word: {corrected_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "HnorCbsUeKqO",
        "outputId": "11aa14fa-d0d0-4f7d-c649-f50aba1da8ab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-235c4a04e010>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0minput_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bannna\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcorrected_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspell_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Original word: {input_word}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Corrected word: {corrected_word}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-235c4a04e010>\u001b[0m in \u001b[0;36mspell_check\u001b[0;34m(word, tokenizer, max_seq_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpredicted_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Convert the sequence back to the word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mcorrected_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrected_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-235c4a04e010>\u001b[0m in \u001b[0;36msequences_to_text\u001b[0;34m(sequence, tokenizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msequences_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Convert the sequence back to the word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36msequences_to_texts\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \"\"\"\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_texts_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msequences_to_texts_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36msequences_to_texts_generator\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    }
  ]
}